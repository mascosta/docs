# Usando o KIND com Cilium

Sauda√ß√µes! Caso voc√™ esteja querendo come√ßar nos estudos de seguran√ßa, envolvendo um cluster kubernetes, aqui vai um breve tutorial de como iniciar esses estudos.

O objetivo desse guia √© construir um cluster, usando uma ferramenta indicada pela galera mantenedora do projeto do Kubernetes (CNCF). Trata-se da ferramenta [KIND](https://kind.sigs.k8s.io/) (Kubernetes IN Docker).

Em adi√ß√£o, para ser permitido o estudo sobre as *NetworkPolicies*, vamos abordar a instala√ß√£o do [Cilium](https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/) como plugin _Container Network Interface (CNI)_.

E pra finalizar, vamos realizar um laborat√≥rio simples, mostrando o funcionamento das ferramentas! :D

Antes de come√ßarmos, √© necess√°rio que o docker esteja instalado na m√°quina. Outros ContainerRuntimes n√£o foram validados. ¬Ø\\\_(„ÉÑ)_/¬Ø


## 1 - Passo: Instala√ß√£o do KIND

A documenta√ß√£o da ferramenta j√° √© muito boa, vou deixar [aqui o link](https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries) pra facilitar a navega√ß√£o.

Dos m√©todos de instala√ß√£o dispon√≠veis, o usado para esse guia foi o de download do bin√°rio:

```bash
# For AMD64 / x86_64
[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.29.0/kind-linux-amd64
# For ARM64
[ $(uname -m) = aarch64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.29.0/kind-linux-arm64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
```

Obs.: Algumas distros linux n√£o usam o ```/usr/local/bin``` na vari√°vel ```${PATH}``` padr√£o. Para checar, basta executar o comando abaixo:

```bash
echo ${PATH}
```

Caso n√£o esteja, basta adicionar a seguinte linha no arquivo ```~/.bashrc```

```bash
(...)
export PATH="$PATH:/usr/local/bin"
(...)
```

E carregar as configura√ß√µes com o comando:

```bash
source ~/.bashrc
```

## 2 - Cria√ß√£o do cluster

Com o bin√°rio baixado, agora √© necess√°ria a cria√ß√£o do cluster, para esse exemplo que n√£o √© padr√£o duas coisas precisam ser levadas em considera√ß√£o:

#### I - A instala√ß√£o n√£o usar√° o CNI padr√£o, ent√£o, √© necess√°rio desabilit√°-lo.


#### II - O Cilium precisa de algumas "permiss√µes" dentro do sistema operacional que o usu√°rio comum n√£o tem, sendo necess√°ria a execu√ß√£o como root nos passos a seguir. 

Obs.: Caso queira contribuir com como fazer essa configura√ß√£o sem usar o usu√°rio root, fique a vontade ;)

### 2.1 - Criando o arquivo de configura√ß√£o

Criar, com o editor de texto de prefer√™ncia, o arquivo ```kind-config.yaml``` com o seguinte conte√∫do:

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: cluster1
nodes:
- role: control-plane
  extraPortMappings: # Portas a serem expostas do cluster
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
- role: worker
- role: worker
- role: worker
networking:
  disableDefaultCNI: true # Desativa√ß√£o do CNI padr√£o
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.245.0.0/16"
```

Obs.: A configura√ß√£o acima usa um control-plane e tr√™s workers, caso julgue interessante, pode reduzir a quantidade de workers para menor uso dos recursos.

### 2.2 - Criando o cluster

Ap√≥s a cria√ß√£o do arquivo de configura√ß√£o com as defini√ß√µes do cluster √© necess√°rio aplicar para subir os containers que executar√£o o cluster.

O comando para cria√ß√£o, usando o arquivo de configura√ß√£o, √© o seguinte:

```bash
kind create cluster --config=kind-config.yaml
```

Depois da cria√ß√£o, ao executar o ```docker ps```, os containers ser√£o mostrados como abaixo:

```bash
CONTAINER ID   IMAGE                  COMMAND                  CREATED       STATUS       PORTS                                                             NAMES
74e6730e90ca   kindest/node:v1.33.1   "/usr/local/bin/entr‚Ä¶"   2 hours ago   Up 2 hours   0.0.0.0:30001-30002->30001-30002/tcp, 127.0.0.1:36843->6443/tcp   cluster1-control-plane
81adc969c9c9   kindest/node:v1.33.1   "/usr/local/bin/entr‚Ä¶"   2 hours ago   Up 2 hours                                                                     cluster1-worker2
d7f5c7b43a6d   kindest/node:v1.33.1   "/usr/local/bin/entr‚Ä¶"   2 hours ago   Up 2 hours                                                                     cluster1-worker
c54aa0d50534   kindest/node:v1.33.1   "/usr/local/bin/entr‚Ä¶"   2 hours ago   Up 2 hours                                                                     cluster1-worker3
```

### 2.3 - Configurando o kubectl

Para manuseio dos objetos dentro desse cluster, √© necess√°ria a instala√ß√£o do bin√°rio kubectl. 

Existem reposit√≥rios de distros RHEL e Debian like e op√ß√£o do uso direto do bin√°rio. Para ambientes produtivos, √© recomendado o uso do gerenciador de pacotes das distros, atrav√©s dos reposit√≥rios. Mas para esse guia, ser√° abordado o uso do bin√°rio simples. O guia oficial pode ser lido atrav√©s desse [link](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/).


```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mv kubectl /usr/local/bin/
```

Ap√≥s a instala√ß√£o, ao executar o comando ```kubectl get pods -A```, que lista todos os *pods* de todos os *namespaces*, ser√° observado o seguinte resultado:

```bash
NAMESPACE            NAME                                             READY   STATUS    RESTARTS   AGE
kube-system          coredns-674b8bbfcf-4kmgg                         0/1     Pending   0          12s
kube-system          coredns-674b8bbfcf-pv9gf                         0/1     Pending   0          12s
kube-system          etcd-cluster1-control-plane                      1/1     Running   0          18s
kube-system          kube-apiserver-cluster1-control-plane            1/1     Running   0          18s
kube-system          kube-controller-manager-cluster1-control-plane   1/1     Running   0          18s
kube-system          kube-proxy-dl2ml                                 1/1     Running   0          10s
kube-system          kube-proxy-qq4vf                                 1/1     Running   0          10s
kube-system          kube-proxy-r9jv2                                 1/1     Running   0          12s
kube-system          kube-proxy-sw24m                                 1/1     Running   0          10s
kube-system          kube-scheduler-cluster1-control-plane            1/1     Running   0          19s
local-path-storage   local-path-provisioner-7dc846544d-7vxzw          0/1     Pending   0          12s
``` 

Evidenciando que alguns pods est√£o em STATUS **Pending**, o que ser√° tratado a seguir.

## 3 - Configurando o Cilium

A "pend√™ncia" do passo anterior se d√° pela n√£o exist√™ncia, ainda, da interface de rede local do cluster, que √© respons√°vel pela comunica√ß√£o entre os objetos.

Para esse quest√£o, basta realizar a instala√ß√£o do cilium que, como em passos anteriores, possui mais de um m√©todo para instala√ß√£o, referenciado no come√ßo desse guia.

Nesse laborat√≥rio que ser√° constru√≠do, ser√° adotada a instala√ß√£o atrav√©s da CLI, que usa o [HELM](https://helm.sh/) em backgroud para a tarefa. Seguem os comandos para instala√ß√£o da CLI:

```bash
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
```
Com a CLI instalada, agora √© necess√°rio instalar o *CNI* no cluster:

```bash
cilium install --version=$(curl -s https://raw.githubusercontent.com/cilium/cilium/refs/heads/main/stable.txt)
```

Depois dessa execu√ß√£o, os status dos pods mudar√£o, alem da cria√ß√£o de pods adicionais para o funcionamento do *CNI*.

```
NAMESPACE            NAME                                             READY   STATUS              RESTARTS   AGE
kube-system          cilium-5lppt                                     0/1     Init:0/6            0          9s
kube-system          cilium-envoy-58z4s                               0/1     ContainerCreating   0          9s
kube-system          cilium-envoy-8t72t                               0/1     ContainerCreating   0          9s
kube-system          cilium-envoy-rc8wn                               0/1     ContainerCreating   0          9s
kube-system          cilium-envoy-tht98                               0/1     ContainerCreating   0          9s
kube-system          cilium-g5rmb                                     0/1     Init:0/6            0          9s
kube-system          cilium-j6d5d                                     0/1     Init:0/6            0          9s
kube-system          cilium-lxnzl                                     0/1     Init:0/6            0          9s
kube-system          cilium-operator-686959b66d-fjj45                 0/1     ContainerCreating   0          9s
kube-system          coredns-674b8bbfcf-4kmgg                         0/1     Pending             0          13m
kube-system          coredns-674b8bbfcf-pv9gf                         0/1     Pending             0          13m
kube-system          etcd-cluster1-control-plane                      1/1     Running             0          13m
kube-system          kube-apiserver-cluster1-control-plane            1/1     Running             0          13m
kube-system          kube-controller-manager-cluster1-control-plane   1/1     Running             0          13m
kube-system          kube-proxy-dl2ml                                 1/1     Running             0          13m
kube-system          kube-proxy-qq4vf                                 1/1     Running             0          13m
kube-system          kube-proxy-r9jv2                                 1/1     Running             0          13m
kube-system          kube-proxy-sw24m                                 1/1     Running             0          13m
kube-system          kube-scheduler-cluster1-control-plane            1/1     Running             0          13m
local-path-storage   local-path-provisioner-7dc846544d-7vxzw          0/1     Pending             0          13m
```
Nos testes efetuados, por volta de sete minutos os pods mudaram o STATUS de **Pending** para **Running**:
```
NAMESPACE            NAME                                             READY   STATUS    RESTARTS   AGE
kube-system          cilium-5lppt                                     1/1     Running   0          8m14s
kube-system          cilium-envoy-58z4s                               1/1     Running   0          8m14s
kube-system          cilium-envoy-8t72t                               1/1     Running   0          8m14s
kube-system          cilium-envoy-rc8wn                               1/1     Running   0          8m14s
kube-system          cilium-envoy-tht98                               1/1     Running   0          8m14s
kube-system          cilium-g5rmb                                     1/1     Running   0          8m14s
kube-system          cilium-j6d5d                                     1/1     Running   0          8m14s
kube-system          cilium-lxnzl                                     1/1     Running   0          8m14s
kube-system          cilium-operator-686959b66d-fjj45                 1/1     Running   0          8m14s
kube-system          coredns-674b8bbfcf-4kmgg                         1/1     Running   0          21m
kube-system          coredns-674b8bbfcf-pv9gf                         1/1     Running   0          21m
kube-system          etcd-cluster1-control-plane                      1/1     Running   0          21m
kube-system          kube-apiserver-cluster1-control-plane            1/1     Running   0          21m
kube-system          kube-controller-manager-cluster1-control-plane   1/1     Running   0          21m
kube-system          kube-proxy-dl2ml                                 1/1     Running   0          21m
kube-system          kube-proxy-qq4vf                                 1/1     Running   0          21m
kube-system          kube-proxy-r9jv2                                 1/1     Running   0          21m
kube-system          kube-proxy-sw24m                                 1/1     Running   0          21m
kube-system          kube-scheduler-cluster1-control-plane            1/1     Running   0          21m
local-path-storage   local-path-provisioner-7dc846544d-7vxzw          1/1     Running   0          21m
```

## 4 - Bonus Track (Completions)

Nesse guia foram abordados 3 bin√°rios principais:

- kind
- kubectl
- cilium

Para adicionar o _bash completion_, ou o completion de outro shell de prefer√™ncia, basta adicionar as linhas abaixo dentro do arquivo ~/.bashrc.

```bash
source <(kind completion bash)
source <(kubectl completion bash)
source <(cilium completion bash)
```

E depois recarregar com o comando ```source ~/.bashrc``` :)

## 5 - Laborat√≥rio simples

J√° com o ambiente configurado, ser√° feita a cria√ß√£o de alguns objetos para valida√ß√£o e cria√ß√£o de pol√≠ticas de rede (*NetworkPolicies*) internas do cluster.

Por padr√£o, **todos** os containers de **todos** os namespaces se comunicam, contudo, e se fosse necess√°rio limitar o acesso do *container A* a apenas membros do *namespace A*?

O cen√°rio para o experimento ser√° constru√≠do da seguinte forma:

| Nome | Namespace | Servi√ßo 
-----|------------|----------
| web-server | ns-a | 80/TCP
| debuga | ns-a | x
| debugb | ns-a | x

O pod **debug1** realizar√° um acesso ao servi√ßo HTTP do pod **web-server** no namespace **ns-a**.

Para o cen√°rio, ser√£o necess√°rios alguns passos, descritos a seguir:

### 5.1 - Cria√ß√£o dos namespaces

No cluster previamente criado, √© necess√°ria a cria√ß√£o dos namespaces, atrav√©s dos comandos:

```bash
kubectl create namespace ns-a 
kubectl create namespace ns-b
```

### 5.2 - Cria√ß√£o dos pods

Ap√≥s a cria√ß√£o dos namespaces, √© necess√°rio criar os pods para valida√ß√£o do ambiente. Esses pods ser√£o criados por meio de deployments, descritos abaixo (**Importante salv√°-los no mesmo diret√≥rio, para facilitar na aplica√ß√£o mais a frente**):

- **WEB-SERVER**

```web-server.yaml```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: web-server
    lab: teste
  name: web-server
  namespace: ns-a
spec:
  selector:
    matchLabels:
      app: web-server
      lab: teste
  template:
    metadata:
      labels:
        app: web-server
        lab: teste
    spec:
      containers:
      - image: httpd
        name: httpd
```
 - **DEBUGA**

```debuga.yaml```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: debuga
    lab: teste
  name: debuga
  namespace: ns-a
spec:
  selector:
    matchLabels:
      app: debuga
      lab: teste
  template:
    metadata:
      labels:
        app: debuga
        lab: teste
    spec:
      containers:
      - command:
        - sleep
        - "200000"
        image: debian
        name: debian
```

- **DEBUGB**

```debugb.yaml```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: debugb
    lab: teste
  name: debugb
  namespace: ns-b
spec:
  selector:
    matchLabels:
      app: debugb
      lab: teste
  template:
    metadata:
      labels:
        app: debugb
        lab: teste
    spec:
      containers:
      - command:
        - sleep
        - "200000"
        image: debian
        name: debian
```

Em seguida, criar os objetos atrav√©s do comando 

```bash
kubectl create -f .
```

### 5.3 - Testando as conex√µes antes da pol√≠tica de rede

Para valida√ß√£o da conex√£o, esperamos duas condi√ß√µes:

- Namespace do servi√ßo: ‚úÖ
- Qualquer namespace: ‚úÖ

Antes de acessar qualquer pod, √© importante checar os endere√ßos de IP de cada pod, para validar as conex√µes. 

```bash
kubectl get pods -A -l lab=teste -o wide
```

A sa√≠da ser√° algo similar ao abaixo:

```bash
NAMESPACE   NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE               NOMINATED NODE   READINESS GATES
ns-a        debuga-c8c59775f-5zvgs        1/1     Running   0          12m   10.244.2.139   cluster1-worker3   <none>           <none>
ns-a        web-server-59cc86b959-lchmd   1/1     Running   0          12m   10.244.3.20    cluster1-worker    <none>           <none>
ns-b        debugb-58f6779fdb-zldw2       1/1     Running   0          12m   10.244.1.231   cluster1-worker2   <none>           <none>
```

Como se pode observar, nesse exemplo, o endere√ßo de IP do pod web-server √© ```10.244.3.20```

Acessar os pods de debug, atrav√©s do comando ```kubectl exec```, que permite, assim como o Docker, execu√ß√£o de um comando dentro do container:

- DEBUGA
```bash
kubectl exec -it -n ns-a deployments/debuga -- bash
```
- DEBUGB
```bash
kubectl exec -it -n ns-b deployments/debugb -- bash
```

Uma vez acessando o terminal do container, executar os seguintes comandos:

```bash
apt update &&
apt install curl -y && \
curl 10.244.3.20
```
Caso o comando traga um retorno similar ao abaixo, a conex√£o se deu de forma esperada, para ambos os pods.
```html
<html><body><h1>It works!</h1></body></html>
```

### 5.4 - Aplicando a pol√≠tica de rede

Como visto no passo anterior, ambos os namespaces tem acesso ao pode que est√° executando o webserver. 

Agora ser√° aplicada uma pol√≠tica de rede que permitir√° apenas acesso do mesmo namespace da aplica√ß√£o, ```ns-a```.

Criar o arquivo com o seguinte conte√∫do:

```np.yaml```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-http
  namespace: ns-a
spec:
  podSelector:
    matchLabels:
      app: web-server
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ns-a
    ports:
    - protocol: TCP
      port: 80
```

Em seguida, criar a pol√≠tica:

```bash
kubectl create -f np.yaml
```

### 5.5 - Testando as conex√µes ap√≥s a pol√≠tica de rede

Da mesma forma que a valida√ß√£o anterior, esperamos duas condi√ß√µes:

- Namespace do servi√ßo: ‚úÖ
- Qualquer namespace: üî¥

Como os pacotes ja foram instalados nos pods no passo 5.3, o comando para valida√ß√£o se dar√° de forma mais simples (**lembrar de alterar o endere√ßo de IP do web-server para seu ambiente**):

- DEBUGA

```bash
kubectl exec -it -n ns-a deployments/debuga -- curl 10.244.3.20
```

- DEBUGB

```bash
kubectl exec -it -n ns-b deployments/debugb -- curl 10.244.3.20
```

Com a execu√ß√£o dos comandos, o ```debuga``` deve trazer o html, j√° o ```debugb``` deve apresentar timeout na requisi√ß√£o. 

```bash

root@server:/# kubectl exec -it -n ns-a deployments/debuga -- curl 10.244.3.20
<html><body><h1>It works!</h1></body></html>

root@server:/# kubectl exec -it -n ns-b deployments/debugb -- curl 10.244.3.20
curl: (28) Failed to connect to 10.244.3.20 port 80 after 130339 ms: Couldn't connect to server
command terminated with exit code 28
```

## 6 - Removendo o cluster

Caso deseje encerrar o ambiente e liberar os recursos utilizados pelos containers do KIND, √© poss√≠vel deletar todo o cluster com um √∫nico comando.

Se voc√™ usou o nome ```cluster1``` no seu arquivo de configura√ß√£o (como no exemplo deste guia), execute:

```bash
kind delete cluster --name cluster1
```

Isso ir√°:

- Remover todos os containers relacionados ao cluster KIND.

- Excluir o contexto do cluster do kubectl config.

- Liberar as portas mapeadas no host.

- Evitar consumo desnecess√°rio de recursos caso voc√™ n√£o esteja mais utilizando o ambiente.

- Caso queira verificar se ainda h√° clusters KIND existentes antes de deletar, use:

```bash
kind get clusters
```

üí° Dica: Sempre remova o cluster quando terminar seus testes, principalmente em m√°quinas com recursos limitados ou ambientes compartilhados.

Bem, por esse guia √© isso, espero que ajude! <o